{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwDQPmsfO7+FeSbVgCEmbf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanat503/IronGPT/blob/main/gpt_Tolstoy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaAde03vs99L",
        "outputId": "a45cd343-d113-4994-c421-4e970bfe34f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Моя видеокарта: Tesla T4\n",
            "--2026-02-26 16:11:06--  https://www.gutenberg.org/cache/epub/2600/pg2600.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3359652 (3.2M) [text/plain]\n",
            "Saving to: ‘modern_text.txt’\n",
            "\n",
            "modern_text.txt     100%[===================>]   3.20M  6.72MB/s    in 0.5s    \n",
            "\n",
            "2026-02-26 16:11:07 (6.72 MB/s) - ‘modern_text.txt’ saved [3359652/3359652]\n",
            "\n",
            "Размер словаря (vocab_size): 112 символов\n",
            "Общий объем текста: 3227578 символов\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "!wget -O modern_text.txt https://www.gutenberg.org/cache/epub/2600/pg2600.txt\n",
        "\n",
        "with open('modern_text.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Размер словаря (vocab_size): {vocab_size} символов\")\n",
        "print(f\"Общий объем текста: {len(text)} символов\")\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "block_size = 256\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size,bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        pass\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # (B, T, head_size)\n",
        "        q = self.query(x) # (B, T, head_size)\n",
        "        v = self.value(x) # (B, T, head_size)\n",
        "\n",
        "        wei = q @ k.transpose(-2,-1) * (k.shape[-1] ** -0.5)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        probs = F.softmax(wei, dim=-1)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        out = probs @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd,n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    self.proj(out)\n",
        "    self.drop = nn.Dropout(dropout)\n",
        "    return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(n_embd*4, n_embd),\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffw = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.sa(self.ln2(x))\n",
        "\n",
        "    return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        logits, loss = self(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "\n",
        "        idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B,T = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.drop(x)\n",
        "    x = self.blocks(x)\n",
        "    self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B, T, C = logits.shape\n",
        "      logits = logits.view(B*T, C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = BigramLanguageModel()\n",
        "\n",
        "    dummy_idx = torch.randint(0, vocab_size, (4, 8)) # B=4, T=8\n",
        "\n",
        "    logits, loss = model(dummy_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFC2WwsftZHH",
        "outputId": "87e415a4-cb39-422a-8ac4-88fc7107eea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Финальный shape логитов: torch.Size([4, 8, 112])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "max_iters = 5000\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    if iter % 300 == 0:\n",
        "        print(f\"Итерация {iter}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"Обучение завершено. Финальный Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUgktU9IuDOe",
        "outputId": "dabefaf6-1ab3-461b-b574-4c3553860590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Итерация 0: Loss = 5.1694\n",
            "Итерация 300: Loss = 2.4926\n",
            "Итерация 600: Loss = 2.0741\n",
            "Итерация 900: Loss = 1.9393\n",
            "Итерация 1200: Loss = 1.8168\n",
            "Итерация 1500: Loss = 1.7388\n",
            "Итерация 1800: Loss = 1.6909\n",
            "Итерация 2100: Loss = 1.6658\n",
            "Итерация 2400: Loss = 1.6131\n",
            "Итерация 2700: Loss = 1.5475\n",
            "Итерация 3000: Loss = 1.4924\n",
            "Итерация 3300: Loss = 1.4925\n",
            "Итерация 3600: Loss = 1.4712\n",
            "Итерация 3900: Loss = 1.4334\n",
            "Итерация 4200: Loss = 1.4425\n",
            "Итерация 4500: Loss = 1.4179\n",
            "Итерация 4800: Loss = 1.4052\n",
            "Обучение завершено. Финальный Loss: 1.3865488767623901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(self, idx, max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "\n",
        "        logits, loss = self(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "UOlxzVTFuKtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "generated_idx = model.generate(context, max_new_tokens=3000)\n",
        "print(decode(generated_idx[0].tolist()))\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4exnAL-suP5H",
        "outputId": "6cc7f5d0-9b66-40d7-d57a-461d65d86428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "e drown by the line brought of his broad-meting only on of the the presing\n",
            "into to tak tone.\n",
            "\n",
            "“Every good something,” she aspemeant, “Oh, Dange Borís,”\n",
            "continued Pierre thing, while Prince Andrew that it sung that\n",
            "he’s eyes this own people dictions. It everything begin weat have closen.\n",
            "\n",
            "Pierre noticed, having the way men for and able the daughter’n and kiss\n",
            "incons. You get great of youpeng it partrical now have joyfulling the\n",
            "ground offs, glassmer for Prally and whispings he lightly as sinn appeor an\n",
            "officer which he noisee continue the militiant was my dicked up for a\n",
            "so chard obtabing frightened troun ober to her many. But what had Pfull\n",
            "articis, and Annatole, which juiced him to be, a sing fear anbuder it\n",
            "nearing the now of One when itt a lies Moscow, as and kissed banet.\n",
            "\n",
            "“What it!” hought, of turning bentration questions, ordears his chers\n",
            "belighted with his heart furent why ceented the dipsortion in the colder\n",
            "kissed to Pierre. Coiclliar little to a\n",
            "succoun of Torme and spokene. He was glomes that ploogly he\n",
            "can heard her turned wours to cept the and for lips. Two recogner from\n",
            "place arappeartuced what he drivenced to kember suseed the good-nature\n",
            "turned joyful whater; youth men that adjuting it arrival might love severance\n",
            "by boldre married.\n",
            "\n",
            "Altaking it at rattby oaccounityuned the concenne batle of French\n",
            "men than it war freend and Austre whole is knowllarms in a more quickly\n",
            "put arting with a man an as with to Rostóv was stranged, beforn the\n",
            "way weaching I was cannon youngging the enemy’s but mest wear haldy, a quourtier\n",
            "of their corown afferor after cormicat. Ihanternýsted years, wity...\n",
            "\n",
            "It wis my presed this in listened you-knound it march. Prince Vasíli was\n",
            "moment youth declor are not rectoat, in and off the change him indicate\n",
            "house to be workn droopporow by the unrstanding it and that old hard to\n",
            "beat Masoppelain Moscow was to his ave mouthing. I\n",
            "have morney, what was suckept and when elyne say:\n",
            "Whell she count to the serf. Whether he knerf than enecessivic\n",
            "was we’ll knew to ramber in hergingster thoughts no whull; having seeming a\n",
            "memblerk, ordis the fot someont clear.\n",
            "\n",
            "That she had wratbited in the countess the ground assiles which he great gent\n",
            "at conclumannder and would angrils in haung, like and comparks, in which he\n",
            "adjutant nevealow cannon of was a ganificultion rest of what had berend him darrs\n",
            "manassmes) reding of charming, as he days approached his always what all\n",
            "that he could most to play. Head cannful seing this cakers hoscle.\n",
            "“My listanishy you win,” she said sole plain. “It was noticed mysking\n",
            "in know and your best,” the question myservance.\n",
            "\n",
            "“Mattch, my dear?” said Be maramis into Kutássyly, say offer whethy you\n",
            "want, was it dicusten to shout ench—pay.\n",
            "\n",
            "“‘I have you so to, Lis?’ ‘You’re Gouard’s?’”\n",
            "Shons, when.’t I lisst, a voi\n",
            "wortury your havim tryuse!” this musketcy smokey and disirc a tables least\n",
            "her hearily from his so means; his feling!”\n",
            "\n",
            "“Waskitum, the order’s you pracious to. Anna Moscow not arrousing his\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BigramLanguageModel(\n",
              "  (token_embedding_table): Embedding(112, 384)\n",
              "  (position_embedding_table): Embedding(256, 384)\n",
              "  (lm_head): Linear(in_features=384, out_features=112, bias=True)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (drop): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffw): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (drop): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiyPJXq-4Ia7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}